# Nav (v1)

Hey, this is a simple attempt at making a Mac feel a little more programmable. The idea is super basic: take a screenshot, let a model describe what it sees, and then do real mouse and keyboard actions on your laptop. It is not magic or perfect. It is just a scrappy project that tries to automate small tasks on a real Mac screen.

If you have ever wanted a bot that can click around the UI like a person, this is my first pass.

## What this project is

- A tiny desktop app with a chat UI
- A Python agent that sees the screen and decides what to do
- A vision pipeline that finds clickable UI elements
- A real action layer that clicks and types on macOS

## What it is not

- A production ready automation system
- A guaranteed reliable agent
- A full OS replacement

## Quick mental model

Think of it like this:

1. You say the goal in the UI
2. The agent takes a screenshot
3. The model decides the next action
4. The action layer clicks or types
5. Repeat until done or stopped

In plain words, it is just a loop of screenshot then think then act.

## Project structure

The app uses both the desktop folder and the v1 root folder.

| Location | Role |
| --- | --- |
| desktop/ | Electron app with the React UI, preload, and main process. This is the app you run. |
| v1 root | Python backend. The Electron app starts agent_server.py, which uses agent.py, vision.py, and typeandclick.py. |

When you launch the app, Electron starts the Python server using the v1 root path. If the Python files are missing, the UI will not work.

## How it works, for real

### 1. The UI and server handshake

The React UI connects to a local WebSocket at ws://127.0.0.1:8765 and streams in steps. The Python server emits status updates, text deltas, and screenshots as the agent works.

### 2. The agent loop

agent.py owns the main loop. Every step it:

- Takes a screenshot
- Runs vision if needed
- Builds a prompt for the model
- Parses the action JSON
- Calls typeandclick.py to execute the action

### 3. The vision pipeline

vision.py is a mix of semantic detection and classical CV. In auto mode it tries a VLM first, then falls back to CV if needed. It also annotates a screenshot with element IDs so the model can point to a target instead of guessing coordinates.

You will see this kind of annotated output in the UI and in the sample files below.

![Annotated UI example](readme/test2_ui_everything_annotated.png)

### 4. The action layer

typeandclick.py is the boring but important part. It uses pyautogui to click, type, scroll, and press keys. It also checks screen size and fails safely if the display is not available.

## Visuals from this repo

These are real files generated by the vision pipeline and included here so you can see what the agent actually sees.

<table>
	<tr>
		<td>
			<img src="readme/flowchart.png" width="360" alt="Nav flowchart" />
		</td>
		<td>
			<strong>What this flowchart means</strong><br />
			It is the core loop. A user goal goes in, the agent grabs a screenshot,
			runs vision to find clickable UI parts, asks the model for the next action,
			then uses the action layer to click or type. After each step it loops and
			checks the screen again. The whole system is just this loop over and over.
		</td>
	</tr>
</table>

![Another annotated UI example](mac_ui_outputs/test_ui_everything_annotated.png)

The JSON files next to those images include the raw box data and labels.

## Why this exists

I am trying to do a small and honest version of computer use automation. A lot of research projects look impressive, but they are often tuned for benchmarks. Here I just want something that works on my laptop, even if it is messy.

## How this compares to OSWorld and UI-TARS

This is the part where I try to be real about scope.

- OSWorld is a benchmark suite. It measures how well agents complete tasks on a fixed set of apps and goals. It is good for research comparison but it is not focused on your actual laptop setup.
- UI-TARS focuses on dataset driven UI reasoning and structured actions. It is more about training and evaluation of UI actions than building a simple local tool.

This project is smaller. It is more like a personal lab tool. The point is not to win a benchmark. The point is to automate small tasks on macOS and learn what breaks.

## Running it locally

You need both the Python backend and the Electron app.

1. Install Python deps in v1 root

```
pip install -r requirements.txt
```

2. Install desktop app deps

```
cd desktop
npm install
```

3. Run the app

```
npm run dev
```

If the UI says it cannot reach the agent server, make sure the Python server can import websockets and that agent_server.py is in the v1 root.

## Useful files to read

- agent_server.py - WebSocket server and run loop orchestration
- agent.py - main agent logic and memory
- vision.py - UI detection and annotation
- typeandclick.py - mouse and keyboard actions
- desktop/ - Electron app and React UI

## Model settings

The app supports multiple model providers and local OpenAI compatible servers.

- modelProvider can be anthropic, openai_compatible, gemini, or zai
- modelBaseUrl can point to Ollama, LM Studio, or vLLM

Examples of local endpoints:

- http://127.0.0.1:11434/v1 for Ollama
- http://127.0.0.1:1234/v1 for LM Studio
- http://127.0.0.1:8000/v1 for vLLM

## Tests

```
python3 -m unittest discover -s tests -p "test*.py" -v
```

## Notes and limitations

- This is macOS only right now
- It can misclick, so do not use it on sensitive tasks
- Vision quality depends on the model and your screen
- It is a toy, but it is a fun and useful one
